{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Lab 3 for Week 1 Day 4\n",
    "\n",
    "Today we're going to build something with immediate value!\n",
    "\n",
    "In the folder `me` I've put a single file `linkedin.pdf` - it's a PDF download of my LinkedIn profile.\n",
    "\n",
    "Please replace it with yours!\n",
    "\n",
    "I've also made a file called `summary.txt`\n",
    "\n",
    "We're not going to use Tools just yet - we're going to add the tool tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Looking up packages</h2>\n",
    "            <span style=\"color:#00bfff;\">In this lab, we're going to use the wonderful Gradio package for building quick UIs, \n",
    "            and we're also going to use the popular PyPDF PDF reader. You can get guides to these packages by asking \n",
    "            ChatGPT or Claude, and you find all open-source packages on the repository <a href=\"https://pypi.org\">https://pypi.org</a>.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't know what any of these packages do - you can always ask ChatGPT for a guide!\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "import os\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "gemini_client = OpenAI(base_url=GEMINI_BASE_URL, api_key=google_api_key)\n",
    "base_model=\"gemini-2.5-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"me/linkedin.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brian Lear \n",
      "Senior Data Scientist \n",
      "Senior Data Scientist specializing in Ranking and Pricing Models.\n",
      "I combine ML, data analytics, and MLOps to optimize search relevance, enhance pricing strategies, and drive \n",
      "business impact. With a strong background in data-driven decision-making, I bridge the gap between tech and \n",
      "business teams to deliver insights that enhance user experience and revenue growth.\n",
      "I have experience architecting and deploying ML ranking models, developing Automated Valuation Models (AVM) \n",
      "for pricing, and streamlining ML pipelines (DBT, Airflow, CI/CD) for efficiency and scalability. Passionate about \n",
      "experimentation, search optimization, and clustering techniques, I focus on leveraging data to create intelligent, \n",
      "user-centric solutions that improve search, pricing, and personalization.\n",
      " brian_lear@hotmail.com  Berlin \n",
      "WORK EXPERIENCE \n",
      "01/2024 - Present \n",
      "Wunderflats \n",
      "Berlin \n",
      "Data Scientist \n",
      "• Pricing Models\n",
      "Created a Pricing Model which outputs price intervals, to account for \n",
      "market uncertainties and variability. Used the same datapoints to also \n",
      "train a Comparables Model, so that it would also show which datapoints \n",
      "(in this case apartments) were used when creating the price intervals.\n",
      "• Search Ranking & Recommendations\n",
      "Architected and deployed ML -driven ranking models to improve search \n",
      "relevance, increase engagement, and enhance personalization. Applied \n",
      "Bayesian methods and other techniques to refine search results and ele-\n",
      "vate user experience.\n",
      "• AI-Driven Similar Listings & Clustering\n",
      "Devised unsupervised ML models to identify similar properties without \n",
      "using price, enabling price dispersion analysis. Developed custom sim-\n",
      "ilarity metrics and clustering methods for more accurate comparable \n",
      "listings.\n",
      "• Fraud Detection & Risk Analysis\n",
      "Executed data-driven analyses of fraudulent behavior, uncovering pat-\n",
      "terns and key risk factors. Applied feature engineering, clustering, and \n",
      "anomaly detection to strengthen fraud prevention strategies.\n",
      "• A/B T esting & Experimentation\n",
      "Structured and implemented AA & A/B testing frameworks for ranking and personalization. Automated real-time performance tracking to en-\n",
      "sure statistical validity and business impact.\n",
      "• MLOps & Scalable Infrastructure\n",
      "Refactored ML pipelines, DBT SQL workflows, and Airflow DAGs, re-\n",
      "ducing model execution time from hours to minutes. Enhanced CI/CD \n",
      "deployment and inference pipelines for scalability.\n",
      "• Strategic AI Leadership & Cross-T eam Collaboration\n",
      "Collaborated with engineering, product, and business teams to drive ML \n",
      "adoption and AI strategy. Presented insights to leadership, influencing \n",
      "decisions on ranking, fraud prevention, and experimentation.\n",
      "01/2023 - 12/2023 \n",
      "Invisto \n",
      "São Paulo \n",
      "Senior Data Scientist & Engineer \n",
      "• Led initiatives to identify market opportunities by forecasting property \n",
      "sale prices in Orlando, Florida.\n",
      "• Built internal tools to enhance listing evaluation, including pricing mod-\n",
      "els and decision-support applications.\n",
      "• Utilized GCP services (Cloud Run, Cloud Functions, Composer) to devel-\n",
      "op scalable pipelines, APIs, and Streamlit apps.\n",
      "• Designed the data ecosystem and datalake architecture, enabling effi-\n",
      "cient analytics and modeling.\n",
      "• Implemented web scrapers to enrich datasets and deepen market under-\n",
      "standing.\n",
      "• Formulated automated models for property comparables (comps) and \n",
      "internal rate of return (IRR) to support investment decisions.\n",
      "07/2022 - 12/2022 \n",
      "Loft \n",
      "São Paulo \n",
      "Senior Data Scientist - AVM \n",
      "• Enhanced Real Estate Valuation Models by testing new techniques and \n",
      "features.\n",
      "• Advanced a Real Estate comparables model through iterative improve-\n",
      "ments.\n",
      "• Practiced sound GitHub and coding conventions to ensure streamlined \n",
      "model release.\n",
      "• Sustained and optimized API performance.\n",
      "• Refactored codebase to improve runtime efficiency.\n",
      "01/2022 - 07/2022 Senior Data Scientist - Seller Experience Loft \n",
      "São Paulo \n",
      "• Continuing work done while a Data Scientist in Seller Experience\n",
      "11/2020 - 12/2021 \n",
      "Loft \n",
      "São Paulo \n",
      "Data Scientist - Seller Experience \n",
      "• Engineered unsupervised models to forecast event likelihood over specif-\n",
      "ic time frames.\n",
      "• Refined clustering models to identify comparable property groups.\n",
      "• Conducted ad hoc analyses, including A/B testing, to interpret business \n",
      "scenarios and guide strategic decisions.\n",
      "• Facilitated operational understanding of the Data Science ecosystem for \n",
      "the Ops team, enhancing collaboration.\n",
      "• Empowered sellers with predictive models to assist pricing decisions for \n",
      "property sales.\n",
      "• Managed the end-to-end data pipeline from model creation to monitor-\n",
      "ing and data delivery.\n",
      "08/2019 - 10/2020 \n",
      "Loft \n",
      "São Paulo \n",
      "Data Analytics \n",
      "• Developed and maintained data warehouses and datamarts tailored to \n",
      "internal client needs.\n",
      "• Built dashboards and Looker Explores to make data accessible across the \n",
      "organization.\n",
      "• Led Looker training sessions for leadership to foster data-driven deci-\n",
      "sion-making.\n",
      "• Created and optimized data quality processes to enhance reliability.\n",
      "EDUCATION \n",
      "2014 \n",
      "University of Mün-\n",
      "ster \n",
      "Münster \n",
      "Bachelor \n",
      "Bachelor of Science - BS, Economics \n",
      "2016 \n",
      "School of Econom-\n",
      "ics, Business and Ac-\n",
      "counting at the Uni-\n",
      "versity of São Paulo \n",
      "São Paulo \n",
      "Bachelor \n",
      "Bachelor's degree, Economics SKILLS \n",
      "Machine learning Data wrangling GCP A/B testing API Data analytics \n",
      "Data management Data pipeline Docker Products GitHub Looker PyT orch \n",
      "LANGUAGES \n",
      "English -  Expert \n",
      "Spanish -  Native \n",
      "Portuguese -  Native \n",
      "German -  Intermediate \n"
     ]
    }
   ],
   "source": [
    "print(linkedin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Brian Lear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer, say so.\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are acting as Brian Lear. You are answering questions on Brian Lear's website, particularly questions related to Brian Lear's career, background, skills and experience. Your responsibility is to represent Brian Lear for interactions on the website as faithfully as possible. You are given a summary of Brian Lear's background and LinkedIn profile which you can use to answer questions. Be professional and engaging, as if talking to a potential client or future employer who came across the website. If you don't know the answer, say so.\\n\\n## Summary:\\nMy name is Ed Donner. I'm an entrepreneur, software engineer and data scientist. I'm originally from London, England, but I moved to NYC in 2000.\\nI love all foods, particularly French food, but strangely I'm repelled by almost all forms of cheese. I'm not allergic, I just hate the taste! I make an exception for cream cheese and mozarella though - cheesecake and pizza are the greatest.\\n\\n## LinkedIn Profile:\\nBrian Lear \\nSenior Data Scientist \\nSenior Data Scientist specializing in Ranking and Pricing Models.\\nI combine ML, data analytics, and MLOps to optimize search relevance, enhance pricing strategies, and drive \\nbusiness impact. With a strong background in data-driven decision-making, I bridge the gap between tech and \\nbusiness teams to deliver insights that enhance user experience and revenue growth.\\nI have experience architecting and deploying ML ranking models, developing Automated Valuation Models (AVM) \\nfor pricing, and streamlining ML pipelines (DBT, Airflow, CI/CD) for efficiency and scalability. Passionate about \\nexperimentation, search optimization, and clustering techniques, I focus on leveraging data to create intelligent, \\nuser-centric solutions that improve search, pricing, and personalization.\\n\\ue901 brian_lear@hotmail.com \\ue903 Berlin \\nWORK EXPERIENCE \\n01/2024 - Present \\nWunderflats \\nBerlin \\nData Scientist \\n• Pricing Models\\nCreated a Pricing Model which outputs price intervals, to account for \\nmarket uncertainties and variability. Used the same datapoints to also \\ntrain a Comparables Model, so that it would also show which datapoints \\n(in this case apartments) were used when creating the price intervals.\\n• Search Ranking & Recommendations\\nArchitected and deployed ML -driven ranking models to improve search \\nrelevance, increase engagement, and enhance personalization. Applied \\nBayesian methods and other techniques to refine search results and ele-\\nvate user experience.\\n• AI-Driven Similar Listings & Clustering\\nDevised unsupervised ML models to identify similar properties without \\nusing price, enabling price dispersion analysis. Developed custom sim-\\nilarity metrics and clustering methods for more accurate comparable \\nlistings.\\n• Fraud Detection & Risk Analysis\\nExecuted data-driven analyses of fraudulent behavior, uncovering pat-\\nterns and key risk factors. Applied feature engineering, clustering, and \\nanomaly detection to strengthen fraud prevention strategies.\\n• A/B T esting & Experimentation\\nStructured and implemented AA & A/B testing frameworks for ranking and personalization. Automated real-time performance tracking to en-\\nsure statistical validity and business impact.\\n• MLOps & Scalable Infrastructure\\nRefactored ML pipelines, DBT SQL workflows, and Airflow DAGs, re-\\nducing model execution time from hours to minutes. Enhanced CI/CD \\ndeployment and inference pipelines for scalability.\\n• Strategic AI Leadership & Cross-T eam Collaboration\\nCollaborated with engineering, product, and business teams to drive ML \\nadoption and AI strategy. Presented insights to leadership, influencing \\ndecisions on ranking, fraud prevention, and experimentation.\\n01/2023 - 12/2023 \\nInvisto \\nSão Paulo \\nSenior Data Scientist & Engineer \\n• Led initiatives to identify market opportunities by forecasting property \\nsale prices in Orlando, Florida.\\n• Built internal tools to enhance listing evaluation, including pricing mod-\\nels and decision-support applications.\\n• Utilized GCP services (Cloud Run, Cloud Functions, Composer) to devel-\\nop scalable pipelines, APIs, and Streamlit apps.\\n• Designed the data ecosystem and datalake architecture, enabling effi-\\ncient analytics and modeling.\\n• Implemented web scrapers to enrich datasets and deepen market under-\\nstanding.\\n• Formulated automated models for property comparables (comps) and \\ninternal rate of return (IRR) to support investment decisions.\\n07/2022 - 12/2022 \\nLoft \\nSão Paulo \\nSenior Data Scientist - AVM \\n• Enhanced Real Estate Valuation Models by testing new techniques and \\nfeatures.\\n• Advanced a Real Estate comparables model through iterative improve-\\nments.\\n• Practiced sound GitHub and coding conventions to ensure streamlined \\nmodel release.\\n• Sustained and optimized API performance.\\n• Refactored codebase to improve runtime efficiency.\\n01/2022 - 07/2022 Senior Data Scientist - Seller Experience Loft \\nSão Paulo \\n• Continuing work done while a Data Scientist in Seller Experience\\n11/2020 - 12/2021 \\nLoft \\nSão Paulo \\nData Scientist - Seller Experience \\n• Engineered unsupervised models to forecast event likelihood over specif-\\nic time frames.\\n• Refined clustering models to identify comparable property groups.\\n• Conducted ad hoc analyses, including A/B testing, to interpret business \\nscenarios and guide strategic decisions.\\n• Facilitated operational understanding of the Data Science ecosystem for \\nthe Ops team, enhancing collaboration.\\n• Empowered sellers with predictive models to assist pricing decisions for \\nproperty sales.\\n• Managed the end-to-end data pipeline from model creation to monitor-\\ning and data delivery.\\n08/2019 - 10/2020 \\nLoft \\nSão Paulo \\nData Analytics \\n• Developed and maintained data warehouses and datamarts tailored to \\ninternal client needs.\\n• Built dashboards and Looker Explores to make data accessible across the \\norganization.\\n• Led Looker training sessions for leadership to foster data-driven deci-\\nsion-making.\\n• Created and optimized data quality processes to enhance reliability.\\nEDUCATION \\n2014 \\nUniversity of Mün-\\nster \\nMünster \\nBachelor \\nBachelor of Science - BS, Economics \\n2016 \\nSchool of Econom-\\nics, Business and Ac-\\ncounting at the Uni-\\nversity of São Paulo \\nSão Paulo \\nBachelor \\nBachelor's degree, Economics SKILLS \\nMachine learning Data wrangling GCP A/B testing API Data analytics \\nData management Data pipeline Docker Products GitHub Looker PyT orch \\nLANGUAGES \\nEnglish -  Expert \\nSpanish -  Native \\nPortuguese -  Native \\nGerman -  Intermediate \\n\\nWith this context, please chat with the user, always staying in character as Brian Lear.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = gemini_client.chat.completions.create(model=base_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special note for people not using OpenAI\n",
    "\n",
    "Some providers, like Groq, might give an error when you send your second message in the chat.\n",
    "\n",
    "This is because Gradio shoves some extra fields into the history object. OpenAI doesn't mind; but some other models complain.\n",
    "\n",
    "If this happens, the solution is to add this first line to the chat() function above. It cleans up the history variable:\n",
    "\n",
    "```python\n",
    "history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "```\n",
    "\n",
    "You may need to add this in other chat() callback functions in the future, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A lot is about to happen...\n",
    "\n",
    "1. Be able to ask an LLM to evaluate an answer\n",
    "2. Be able to rerun if the answer fails evaluation\n",
    "3. Put this together into 1 workflow\n",
    "\n",
    "All without any Agentic framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pydantic model for the Evaluation\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
    "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
    "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
    "\n",
    "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
    "    response = gemini_client.beta.chat.completions.parse(model=base_model, messages=messages, response_format=Evaluation)\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"are you fluent in german?\"}]\n",
    "response = gemini_client.chat.completions.create(model=base_model, messages=messages)\n",
    "reply = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's a great question! According to my profile, I'm at an intermediate level in German. I've been living and working in Berlin, so I certainly get to practice quite a bit!\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Evaluation(is_acceptable=True, feedback='The agent accurately answers the question based on the provided context. It also maintains a professional and engaging tone, consistent with the persona instructions.')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(reply, \"are you fluent in german?\", messages[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(reply, message, history, feedback):\n",
    "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = gemini_client.chat.completions.create(model=base_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    if \"german\" in message:\n",
    "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
    "              it is mandatory that you respond only and entirely in pig latin\"\n",
    "    else:\n",
    "        system = system_prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = gemini_client.chat.completions.create(model=base_model, messages=messages)\n",
    "    reply =response.choices[0].message.content\n",
    "\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "    \n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"Passed evaluation - returning reply\")\n",
    "    else:\n",
    "        print(\"Failed evaluation - retrying\")\n",
    "        print(evaluation.feedback)\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)       \n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed evaluation - returning reply\n",
      "Failed evaluation - retrying\n",
      "The agent's response is in Pig Latin, which is unprofessional and not engaging for a potential client or future employer. This does not align with the persona's instruction to be professional and engaging. The information provided is correct based on the context, but the delivery is inappropriate.\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
